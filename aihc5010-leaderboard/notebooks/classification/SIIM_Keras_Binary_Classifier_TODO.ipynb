{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Binary Classifier using Keras\n",
    "\n",
    "## SIIM Github Repo\n",
    "https://github.com/ImagingInformatics/machine-learning/blob/master/Education/KerasBinaryClassifier/SIIM_Keras_Binary_Classifier.ipynb\n",
    "\n",
    "### Task \n",
    "\n",
    "Train a deep learning model to classify sagittal T1 MRI sequences into pre- or post-contrast.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "1. Basic understanding of machine learning and deep learning\n",
    "2. Programming in Python\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "At the end of this activity, you will be able to:\n",
    "\n",
    "1. Understand how to organize your data to use it to train a deep learning model\n",
    "2. Use the standard data handler from Keras (DataGenerator) to access your dataset to train a model\n",
    "3. Create a custom convolutional neural network\n",
    "4. Train a model\n",
    "5. Calculate metrics in the Validation and Test sets\n",
    "\n",
    "\n",
    "### Acknowledgements\n",
    "\n",
    "This Jupyter Notebook was based on code by Paulo Eduardo de Aguiar Kuriki (paulokuriki@gmail.com), modified by Felipe Kitamura (kitamura.felipe@gmail.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# TODO: install gdown and restart runtime\n",
    "\n",
    "Run the cell below. Then depending on whether you are running this in a jupyter notebook or on Google Colab follow these instructions:\n",
    "\n",
    "**Jupyter** - click on the 'Kernel' tab at the top of the notebook and choose 'Restart & Clear Output'. Rerun all the cells from the start of the notebook to this point before moving on.\n",
    "\n",
    "**Colab** - click on the 'Runtime' tab at the top of the notebook and choose 'Restart Runtime'. Rerun all the cells from the start of the notebook to this point before moving on.\n",
    "\n",
    "These steps ensure that the installation takes effect. We will use gdown to download the data set shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U --no-cache-dir gdown --pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Setting your Team Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your Team Name below\n",
    "team = # TODO: choose a team name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Your Team Name is:\", team)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Format\n",
    "\n",
    "First of all, we need to split out images into training, validation and test sets\n",
    "\n",
    "For this task, we have a separate test folder and a training folder. The training folder will be split into training and validation by our code.\n",
    "\n",
    "The files are organized in the following folder structure:\n",
    "\n",
    "#### Train/with_gad/ - contains the files of sequences with contrast\n",
    "\n",
    "#### Train/no_gad/ - contains the files of sequences without contrast\n",
    "\n",
    "#### Test/with_gad/ - contains the files of sequences with contrast\n",
    "\n",
    "#### Test/no_gad/ - contains the files of sequences without contrast\n",
    "\n",
    "\n",
    "\n",
    "First thing we need to do is to download the dataset. **This will take a few minutes. Read ahead while you wait.**  \n",
    "\n",
    "Then we unzip our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gdown\n",
    "\n",
    "if not os.path.exists('Train.zip'): # check if already downloaded\n",
    "    gdown.download('https://drive.google.com/uc?id=1rffWXRBaePSo7JMwJm1ygDdpgs7FZuTi', 'Train.zip', quiet=False)\n",
    "    !unzip Train.zip\n",
    "if not os.path.exists('Test.zip'):\n",
    "    gdown.download('https://drive.google.com/uc?id=1x4LTeyPgLNndsP8w0rtn-pCV0LzOVc1H', 'Test.zip', quiet=False)\n",
    "    !unzip Test.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If the automatic download fails. You can manually grab the data sets from these links:\n",
    "\n",
    "https://drive.google.com/file/d/1rffWXRBaePSo7JMwJm1ygDdpgs7FZuTi/view?usp=sharing\n",
    "\n",
    "https://drive.google.com/file/d/1x4LTeyPgLNndsP8w0rtn-pCV0LzOVc1H/view?usp=sharing\n",
    "\n",
    "If you manually downloaded the data sets uncomment the two lines below to unzip them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip Train.zip\n",
    "# !unzip Test.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Input\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def sub_model(team, hyperparam):\n",
    "  url='https://aihc5010.pythonanywhere.com/submit-model/'\n",
    "  hyperparam['team']=team\n",
    "  hyperparam['ModelKey']='lab2'\n",
    "  x=requests.post(url,data=hyperparam)\n",
    "  if x.status_code==200:\n",
    "      print(f\"Model Submitted Successfully for team {team}\")\n",
    "  else:\n",
    "      print(x.status_code)\n",
    "      print(x.text)\n",
    "      print(\"Failed to Submit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the folders where our dataset is saved.\n",
    "TRAIN_DATA_DIR = './Train/'\n",
    "TEST_DATA_DIR = './Test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise some training set examples\n",
    "\n",
    "### with contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "for i,f in enumerate(os.listdir(os.path.join(TRAIN_DATA_DIR, 'with_gad'))[:9]):\n",
    "    # make sure the file is a .png image\n",
    "    if not f.endswith('.png'):\n",
    "        continue\n",
    "    img = mpimg.imread(os.path.join(TRAIN_DATA_DIR, 'with_gad', f))\n",
    "    ax = fig.add_subplot(3,3,i+1)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "for i,f in enumerate(os.listdir(os.path.join(TRAIN_DATA_DIR, 'no_gad'))[:9]):\n",
    "    # make sure the file is a .png image\n",
    "    if not f.endswith('.png'):\n",
    "        continue\n",
    "    img = mpimg.imread(os.path.join(TRAIN_DATA_DIR, 'no_gad', f))\n",
    "    ax = fig.add_subplot(3,3,i+1)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Number of training and test set examples of each class\n",
    "\n",
    "#### Verify the number of examples available for the training and test sets. Take as many lines of code as you need. Hint: check the visualisation cells above for an example of iterating over all files in a directory that end with a .png extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training_with_gad =  # TODO: calculate the number of training examples with contrast\n",
    "print('Number of training examples with contrast: {}'.format(n_training_with_gad))\n",
    "n_training_no_gad =  # TODO: calculate the number of training examples without contrast\n",
    "print('Number of training examples without contrast: {}'.format(n_training_no_gad))\n",
    "n_training = n_training_with_gad + n_training_no_gad\n",
    "print('Number of training examples : {}\\n'.format(n_training))\n",
    "\n",
    "n_test_with_gad =  # TODO: calculate the number of test examples with contrast\n",
    "print('Number of test examples with contrast: {}'.format(n_test_with_gad))\n",
    "n_test_no_gad =  # TODO: calculate the number of test examples without contrast\n",
    "print('Number of test examples without contrast: {}'.format(n_test_no_gad))\n",
    "n_test = n_test_with_gad + n_test_no_gad\n",
    "print('Number of test examples : {}'.format(n_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making some choices\n",
    "\n",
    "From the visualisations above we can see that the images are different sizes. For the most part either 300-by-300 or 375-by-375 pixels.\n",
    "\n",
    "We will resize the images to 128-by-128. We will see how the images are resized later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the input size of our neural network.\n",
    "# Images will be resized automatically.\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also choose a batch size for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the size of the batch to leverage\n",
    "# the capacity GPUs have to parallelize \n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Training steps per epoch excercise\n",
    "\n",
    "#### Calculate how many steps per epoch there will be during training. Use your chosen batch size and assume that *80% of the training set* will be used for training and 20% for validation. We will verify this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch =  # TODO: given your choice of batch size calculate the number of training steps per epcoh to expect\n",
    "print('Steps per epoch : {}'.format(steps_per_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a function to plot the learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function plots the learning curves, which includes\n",
    "# the loss curve and the accuracy curve over the epochs.\n",
    "def plot_learning_curves(history):\n",
    "    # plot loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title('Binary Cross Entropy Loss')\n",
    "    plt.plot(history.history['loss'], color='blue', label='Train')\n",
    "    plt.plot(history.history['val_loss'], color='orange', label='Validation')\n",
    "    plt.legend(loc='upper right', shadow=True, fontsize='x-large')\n",
    "    plt.show()\n",
    "    # plot accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title('Binary Classification Accuracy')\n",
    "    plt.plot(history.history['accuracy'], color='blue', label='Train')\n",
    "    plt.plot(history.history['val_accuracy'], color='orange', label='Validation')\n",
    "    plt.legend(loc='lower right', shadow=True, fontsize='x-large')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a custom convolutional neral network\n",
    "\n",
    "In the cell below we have provided an example CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every model needs to have an input\n",
    "x1 = Input(shape=(IMG_HEIGHT,IMG_WIDTH,3))\n",
    "\n",
    "# Below, we add paired convolutional and maxpooling layers\n",
    "x = Conv2D(16, (3,3), activation='relu')(x1)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(16, (3,3), activation='relu')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "\n",
    "# Then we flatten the last vector\n",
    "flat1 = Flatten()(x)\n",
    "# Insert a dropout layer with 20% probability\n",
    "flat2 = Dropout(0.2)(flat1)\n",
    "# Then a dense layer\n",
    "class1 = Dense(32, activation='relu', kernel_initializer='he_uniform')(flat2)\n",
    "# And the output layer\n",
    "class1b = Dense(1, activation='linear')(class1)\n",
    "# The output needs to be binary, so we apply a sigmoid function\n",
    "output = sigmoid(class1b)\n",
    "\n",
    "# Here is where the model is created based on the input and output define above\n",
    "model = Model(inputs=x1, outputs=output)\n",
    "\n",
    "# We choose an optimizer\n",
    "opt = Adam(lr=5e-6)\n",
    "\n",
    "# The last step is to compile the model\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# We can see the structure and number of parameter of our network\n",
    "# by calling .summary()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we create a Data Generator, which is a function to read the images from the folders and use them to train and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                   validation_split=0.2)  # set validation split\n",
    "\n",
    "train_it = train_datagen.flow_from_directory(\n",
    "    TRAIN_DATA_DIR,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH), # Here is where we use the image dimensions you chose above\n",
    "    batch_size=BATCH_SIZE,  # Here is where we use the batch size you chose above\n",
    "    class_mode='binary',\n",
    "    color_mode='rgb',\n",
    "    subset='training')  # set as training data\n",
    "\n",
    "val_it = train_datagen.flow_from_directory(\n",
    "    TRAIN_DATA_DIR,  # same directory as training data\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),  # Here is where we use the image dimensions you chose above\n",
    "    batch_size=BATCH_SIZE,  # Here is where we use the batch size you chose above\n",
    "    class_mode='binary',\n",
    "    color_mode='rgb',\n",
    "    subset='validation')  # set as validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show time. This is the moment we train our network\n",
    "\n",
    "As the model trains verify that the number of training steps per epoch matches your calculation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The .fit() method is used to train our network\n",
    "# You can specify here the number os epochs\n",
    "history = model.fit(train_it, steps_per_epoch=len(train_it), \n",
    "                              validation_data=val_it, validation_steps=len(val_it), \n",
    "                              epochs=15, verbose=1)\n",
    "\n",
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Interpreting the learning curves.\n",
    "\n",
    "#### What problems can we identify with this model? What changes could you make to remedy these? Hints: Are we in the high-bias or high-variance regime? What about the rate of convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: convert this cell to markdown and replace this comment with your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Interpreting the differences between the loss and accuracy curves.\n",
    "\n",
    "#### What difference do you notice between the loss and accuracy curves? How can you explain any differences? Hint: What is the difference in what each is measuring?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: convert this cell to markdown and replace this comment with your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can save our trained model in a file so we can restore it to be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save your model, uncomment the following line and run this cell.\n",
    "\n",
    "model.save('SimpleGadClass.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following line allows us to read the model we trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the file name you try to read from is the same you saved\n",
    "\n",
    "model = load_model('SimpleGadClass.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we predict the validation set so we can use both predictions and ground truth to calculate the performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_and_preditions(data_iterator, model):\n",
    "    i=0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    x_ = []\n",
    "    \n",
    "    for x, y in data_iterator:\n",
    "        y_true.extend(y)\n",
    "        y_pred.extend(model.predict(x))\n",
    "        x_.extend(x)\n",
    "        i+=1\n",
    "        if i==len(data_iterator):\n",
    "            break\n",
    "    \n",
    "    y_pred = np.asarray(y_pred)\n",
    "    x_ = np.asarray(x_)\n",
    "    return y_true, y_pred, x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_it = train_datagen.flow_from_directory(\n",
    "    TRAIN_DATA_DIR,  # same directory as training data\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    color_mode='rgb',\n",
    "    subset='validation')  # set as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred, _ = get_labels_and_preditions(val_it, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we plot the ROC curve for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_true, y_pred)\n",
    "\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='AUC (area = {:.5f})'.format(auc_keras), color='orange')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Why might AUC be better than accuracy as a metric for this data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: convert this cell to markdown and replace this comment with your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we plot the confusion matrix for the validation set with a decision threshold of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.5\n",
    "cm = confusion_matrix(y_true, y_pred > thresh)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, cm[i, j],\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > 120 else \"black\", size='x-large')\n",
    "plt.xticks([], [])\n",
    "plt.yticks([], [])\n",
    "plt.title('Confusion matrix ')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we find the threshold with the best  validation set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr_list = []\n",
    "acc_list = []\n",
    "for _th in range(100):\n",
    "    _th = _th / 100.\n",
    "    thr_list.append(_th)\n",
    "    acc_list.append(accuracy_score(y_true, y_pred > _th))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(thr_list,acc_list)\n",
    "plt.plot(thr_list[acc_list.index(max(acc_list))], max(acc_list), 'r+')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "thresh = thr_list[acc_list.index(max(acc_list))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Why do we select the threshold on the validation set and not the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: convert this cell to markdown and replace this comment with your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we plot the confusion matrix for the validation set with the decision threshold we just calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred > thresh)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, cm[i, j],\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > 120 else \"black\", size='x-large')\n",
    "plt.xticks([], [])\n",
    "plt.yticks([], [])\n",
    "plt.title('Confusion matrix ')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we predict the test set so we can use both predictions and ground truth to calculate the performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "test_it = test_datagen.flow_from_directory(\n",
    "    TEST_DATA_DIR,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    color_mode='rgb')  # set as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred, x_test = get_labels_and_preditions(test_it, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we plot the ROC curve for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_true, y_pred)\n",
    "\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='AUC (area = {:.5f})'.format(auc_keras), color='orange')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we plot the confusion matrix for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred > thresh) # apply the threshold we selected using the validation set\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, cm[i, j],\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > 120 else \"black\", size='x-large')\n",
    "plt.xticks([], [])\n",
    "plt.yticks([], [])\n",
    "plt.title('Confusion matrix ')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, we plot a sample of the cases that our model predicted incorrectly so we can understand the errors and try to come up with solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "limit=30\n",
    "counter = 0\n",
    "for i in range(len(y_true)):\n",
    "    if counter >= limit:\n",
    "        break\n",
    "    if y_true[i] != 1. * (y_pred[i, 0] > thresh):\n",
    "        print('Truth:' + str(y_true[i]))\n",
    "        print('Pred:' + str(y_pred[i, 0]))\n",
    "        print(i)\n",
    "        plt.figure(figsize=(7,7))\n",
    "        plt.imshow(x_test[i])\n",
    "        plt.show()\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Are there any reasons you can see that might explain why some images might have failed?  As a reminder a true label of 0 is no contrast a true label of 1 is with contrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: convert this cell to markdown and replace this comment with your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now it's your turn!\n",
    "\n",
    "## TODO: Experiment with the model architecture and training strategy to optimize performance.\n",
    "\n",
    "* You can read the documentation for the layerswe use here https://tensorflow.org/api_docs/python/tf/keras/layers. You couls also think about incorperating batch-norm layers for example.\n",
    "* For the ImageDataGenerator we have added some examples of additional arguments (currently commented out) that can be used for data augmentation. You can read more here https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "* Options for alternate optimizers are here https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "\n",
    "## See how well your modifications compare on the leaderboard here:\n",
    "\n",
    "### https://AIHC5010.pythonanywhere.com/leaderboard-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "### BUILDING THE MODEL ###\n",
    "##########################\n",
    "\n",
    "# TODO: Experiment with the the hyperparameters\n",
    "\n",
    "hyperparam = {\n",
    "    'LearningRate': 5e-4,\n",
    "    'BatchSize': 32,\n",
    "    'Epochs': 15,\n",
    "    'ImageSize': 331,\n",
    "    'Dropout': 0.2\n",
    "}\n",
    "\n",
    "# Every model needs to have an input\n",
    "IMG_HEIGHT = hyperparam['ImageSize']\n",
    "IMG_WIDTH = hyperparam['ImageSize']\n",
    "BATCH_SIZE = hyperparam['BatchSize']\n",
    "\n",
    "x1 = Input(shape=(IMG_HEIGHT,IMG_WIDTH,3))\n",
    "\n",
    "# TODO: Experiment with the model architecture. Add or remove layers.\n",
    "\n",
    "# Below, we add paired convolutional and maxpooling layers\n",
    "x = Conv2D(16, (3,3), activation='relu')(x1)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(16, (3,3), activation='relu')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(32, (3,3), activation='relu')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(32, (3,3), activation='relu')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(64, (3,3), activation='relu')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(64, (3,3), activation='relu')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "\n",
    "# Then we flatten the last vector\n",
    "flat1 = Flatten()(x)\n",
    "# Insert a dropout layer with 20% probability\n",
    "flat2 = Dropout(hyperparam['Dropout'])(flat1)\n",
    "# Then a dense layer\n",
    "class1 = Dense(64, activation='relu', kernel_initializer='he_uniform')(flat2)\n",
    "# And the output layer\n",
    "class1b = Dense(1, activation='linear')(class1)\n",
    "# The output needs to be binary, so we apply a sigmoid function\n",
    "output = sigmoid(class1b)\n",
    "\n",
    "# Here is where the model is created based on the input and output define above\n",
    "model = Model(inputs=x1, outputs=output)\n",
    "# We can see the structure and number of parameter of our network\n",
    "# by calling .summary()\n",
    "model.summary()\n",
    "\n",
    "\n",
    "######################################\n",
    "### DEFINING THE TRAINING STRATEGY ###\n",
    "######################################\n",
    "\n",
    "# TODO: Experiment with the training strategy. Choose data augmentation options or alter the optimizer.\n",
    "\n",
    "# We choose an optimizer\n",
    "opt = Adam(lr=hyperparam['LearningRate'])\n",
    "\n",
    "# The last step is to compile the model\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                   #shear_range=0.2,\n",
    "                                   #zoom_range=0.2,\n",
    "                                   #horizontal_flip=False,\n",
    "                                   #vertical_flip=True,\n",
    "                                   #rotation_range=0,\n",
    "                                   #fill_mode='constant',\n",
    "                                   #cval=0,\n",
    "                                   #preprocessing_function=preprocess_input,\n",
    "                                   validation_split=0.2)  # set validation split\n",
    "\n",
    "train_it = train_datagen.flow_from_directory(\n",
    "    TRAIN_DATA_DIR,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    color_mode='rgb',\n",
    "    subset='training')  # set as training data\n",
    "\n",
    "val_it = train_datagen.flow_from_directory(\n",
    "    TRAIN_DATA_DIR,  # same directory as training data\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    color_mode='rgb',\n",
    "    subset='validation')  # set as validation data\n",
    "\n",
    "history = model.fit(train_it, steps_per_epoch=len(train_it), \n",
    "                              validation_data=val_it, validation_steps=len(val_it), \n",
    "                              epochs=hyperparam['Epochs'], verbose=1)\n",
    "\n",
    "plot_learning_curves(history)\n",
    "\n",
    "\n",
    "######################\n",
    "### SAVE THE MODEL ###\n",
    "######################\n",
    "\n",
    "model.save('SimpleGadClass_vesion2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model and submit to the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "### LOAD THE TRAINED MODEL ###\n",
    "##############################\n",
    "\n",
    "model = load_model('SimpleGadClass_vesion2.h5')\n",
    "\n",
    "\n",
    "############################################\n",
    "### CALCULATE VALIDATION SET PERFORMANCE ###\n",
    "############################################\n",
    "\n",
    "print('############################################')\n",
    "print('### CALCULATE VALIDATION SET PERFORMANCE ###')\n",
    "print('############################################')\n",
    "\n",
    "val_it = train_datagen.flow_from_directory(\n",
    "    TRAIN_DATA_DIR,  # same directory as training data\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    color_mode='rgb',\n",
    "    subset='validation')  # set as validation data\n",
    "\n",
    "y_true, y_pred, _ = get_labels_and_preditions(val_it, model)\n",
    "\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_true, y_pred)\n",
    "\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='AUC (area = {:.5f})'.format(auc_keras), color='orange')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "thresh = 0.5\n",
    "cm = confusion_matrix(y_true, y_pred > thresh)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, cm[i, j],\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > 120 else \"black\", size='x-large')\n",
    "plt.xticks([], [])\n",
    "plt.yticks([], [])\n",
    "plt.title('Confusion matrix threshold={:.3f}'.format(thresh))\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "thr_list = []\n",
    "acc_list = []\n",
    "for _th in range(100):\n",
    "    _th = _th / 100.\n",
    "    thr_list.append(_th)\n",
    "    acc_list.append(accuracy_score(y_true, y_pred > _th))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(thr_list,acc_list)\n",
    "plt.plot(thr_list[acc_list.index(max(acc_list))], max(acc_list), 'r+')\n",
    "plt.show()\n",
    "\n",
    "thresh = thr_list[acc_list.index(max(acc_list))]\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred > thresh)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, cm[i, j],\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > 120 else \"black\", size='x-large')\n",
    "plt.xticks([], [])\n",
    "plt.yticks([], [])\n",
    "plt.title('Confusion matrix threshold={:.3f}'.format(thresh))\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "######################################\n",
    "### CALCULATE TEST SET PERFORMANCE ###\n",
    "######################################\n",
    "\n",
    "print('######################################')\n",
    "print('### CALCULATE TEST SET PERFORMANCE ###')\n",
    "print('######################################')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "test_it = test_datagen.flow_from_directory(\n",
    "    TEST_DATA_DIR,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    color_mode='rgb')  # set as training data\n",
    "\n",
    "y_true, y_pred, x_test = get_labels_and_preditions(test_it, model)\n",
    "\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_true, y_pred)\n",
    "\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='AUC (area = {:.5f})'.format(auc_keras), color='orange')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred > thresh)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, cm[i, j],\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > 120 else \"black\", size='x-large')\n",
    "plt.xticks([], [])\n",
    "plt.yticks([], [])\n",
    "plt.title('Confusion matrix threshold={:.3f}'.format(thresh))\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    if y_true[i] != 1. * (y_pred[i, 0] > thresh):\n",
    "        print('Truth:' + str(y_true[i]))\n",
    "        print('Pred:' + str(y_pred[i, 0]))\n",
    "        print(i)\n",
    "        plt.figure(figsize=(7,7))\n",
    "        plt.imshow(x_test[i])\n",
    "        plt.show()\n",
    "\n",
    "########################\n",
    "### SUBMIT THE MODEL ###\n",
    "########################\n",
    "\n",
    "hyperparam['metric'] = auc_keras\n",
    "sub_model(team, hyperparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
